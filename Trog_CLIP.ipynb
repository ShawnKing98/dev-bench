{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sunny\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\sunny\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\sunny\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookingWhileListeningDataset(Dataset):\n",
    "    def __init__(self, image_folder, pair_csv, transform=None, text_transform=None, k=4):\n",
    "        self.image_folder = image_folder\n",
    "        self.pairs = pd.read_csv(pair_csv)\n",
    "        self.transform = transform if transform is not None else transforms.ToTensor()\n",
    "        self.text_transform = text_transform\n",
    "        self.k = k\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.pairs.iloc[idx]\n",
    "        images = []\n",
    "        text = row['text1']\n",
    "        if self.text_transform:\n",
    "            text = self.text_transform(text)\n",
    "        # Only one text for all k images\n",
    "        for i in range(1, self.k + 1):  # Adjust index to start at 1\n",
    "            image_filename = row[f'image{i}']  # Adjusted to directly use i for indexing\n",
    "            image_path = os.path.join(self.image_folder, image_filename)\n",
    "            image = Image.open(image_path)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)  # This will convert the PIL image to a tensor\n",
    "            images.append(image)\n",
    "        return images, text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, encoder, text_encoder, fusion_module, k=4):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.encoder = encoder  # For images\n",
    "        self.text_encoder = text_encoder  # For text\n",
    "        self.fusion_module = fusion_module\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, images, text):\n",
    "        # Encode each image using the image encoder\n",
    "        image_encodings = [self.encoder(image) for image in images]\n",
    "        # Encode the text only once since there's only one text for all images\n",
    "        text_encoding = self.text_encoder(text) if self.text_encoder else None\n",
    "        # If text_encoder is None, replicate None k times for the loop below\n",
    "        text_encodings = [text_encoding] * self.k if text_encoding is not None else [None] * self.k\n",
    "        # Fuse each image encoding with the text encoding\n",
    "        fused_encodings = [self.fusion_module(image_enc, text_enc) for image_enc, text_enc in zip(image_encodings, text_encodings)]\n",
    "        return fused_encodings\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_folder = \"C:\\\\Users\\\\sunny\\\\Desktop\\\\research\\\\cogeval\\\\sample_items\\\\trog-ex\\\\\"\n",
    "\n",
    "pair_csv = \"C:\\\\Users\\\\sunny\\\\Desktop\\\\research\\\\cogeval\\\\sample_items\\\\trog-ex\\\\trog-manifest.csv\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LookingWhileListeningDataset(\n",
    "    image_folder= image_folder,\n",
    "    pair_csv= pair_csv,\n",
    "    transform=None, \n",
    "    text_transform=None,\n",
    "    k=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(batch):\n",
    "    images = [image for data in batch for image in data[0]]\n",
    "    # Since there is only one text per set of images, we don't need to flatten a list of texts\n",
    "    texts = [data[1] for data in batch]\n",
    "    return images, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=False, collate_fn=collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_softmax_probs(data_loader, processor, model):\n",
    "    model.eval()  \n",
    "    softmax_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, text in data_loader:  # Notice how we're expecting a single text here\n",
    "            # Repeat the single text k times (k = number of images per set)\n",
    "            texts = [text] * len(images)\n",
    "            # Process the images and texts through the CLIP processor\n",
    "            inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "            outputs = model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "            probs = logits_per_image.softmax(dim=1)  # Softmax to get probabilities\n",
    "            softmax_probs.extend(probs)\n",
    "    \n",
    "    return torch.stack(softmax_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to infer channel dimension format",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m softmax_probs \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_softmax_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m, in \u001b[0;36mcalculate_softmax_probs\u001b[1;34m(data_loader, processor, model)\u001b[0m\n\u001b[0;32m      8\u001b[0m texts \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(images)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Process the images and texts through the CLIP processor\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     12\u001b[0m logits_per_image \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits_per_image  \u001b[38;5;66;03m# Image-text similarity score\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sunny\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\clip\\processing_clip.py:103\u001b[0m, in \u001b[0;36mCLIPProcessor.__call__\u001b[1;34m(self, text, images, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m     encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mpixel_values\n",
      "File \u001b[1;32mc:\\Users\\sunny\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\image_processing_utils.py:546\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[1;34m(self, images, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[0;32m    545\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sunny\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\clip\\image_processing_clip.py:282\u001b[0m, in \u001b[0;36mCLIPImageProcessor.preprocess\u001b[1;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like you are trying to rescale already rescaled images. If the input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m     )\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_data_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;66;03m# We assume that all images have the same channel dimension format.\u001b[39;00m\n\u001b[1;32m--> 282\u001b[0m     input_data_format \u001b[38;5;241m=\u001b[39m \u001b[43minfer_channel_dimension_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m    285\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39msize, resample\u001b[38;5;241m=\u001b[39mresample, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[0;32m    288\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\sunny\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\image_utils.py:189\u001b[0m, in \u001b[0;36minfer_channel_dimension_format\u001b[1;34m(image, num_channels)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mshape[last_dim] \u001b[38;5;129;01min\u001b[39;00m num_channels:\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChannelDimension\u001b[38;5;241m.\u001b[39mLAST\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to infer channel dimension format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to infer channel dimension format"
     ]
    }
   ],
   "source": [
    "softmax_probs = calculate_softmax_probs(data_loader, processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0134, 0.0098, 0.0021, 0.0070, 0.0070, 0.0047, 0.0021, 0.0043, 0.0014,\n",
       "         0.0091, 0.0115, 0.0070, 0.0098, 0.0021, 0.0022, 0.0071, 0.0115, 0.0115,\n",
       "         0.0014, 0.0115, 0.0021, 0.0134, 0.0021, 0.0069, 0.0070, 0.0147, 0.0098,\n",
       "         0.0036, 0.0021, 0.0043, 0.0014, 0.0091, 0.0115, 0.0070, 0.0098, 0.0134,\n",
       "         0.0021, 0.0069, 0.0070, 0.0147, 0.0098, 0.0036, 0.0021, 0.0043, 0.0014,\n",
       "         0.0091, 0.0115, 0.0070, 0.0098, 0.0021, 0.0022, 0.0071, 0.0115, 0.0115,\n",
       "         0.0014, 0.0115, 0.0021, 0.0134, 0.0098, 0.0021, 0.0070, 0.0070, 0.0047,\n",
       "         0.0021, 0.0043, 0.0014, 0.0091, 0.0115, 0.0070, 0.0098, 0.0117, 0.0012,\n",
       "         0.0014, 0.0021, 0.0117, 0.0134, 0.0070, 0.0070, 0.0014, 0.0091, 0.0021,\n",
       "         0.0043, 0.0014, 0.0091, 0.0115, 0.0070, 0.0098, 0.0021, 0.0012, 0.0147,\n",
       "         0.0036, 0.0115, 0.0021, 0.0117, 0.0012, 0.0014, 0.0021, 0.0115, 0.0012,\n",
       "         0.0070, 0.0091, 0.0117, 0.0014, 0.0091, 0.0021, 0.0043, 0.0014, 0.0091,\n",
       "         0.0115, 0.0070, 0.0098, 0.0117, 0.0012, 0.0014, 0.0021, 0.0115, 0.0012,\n",
       "         0.0070, 0.0091, 0.0117, 0.0014, 0.0091, 0.0021, 0.0043, 0.0014, 0.0091,\n",
       "         0.0115, 0.0070, 0.0098, 0.0021, 0.0012, 0.0147, 0.0036, 0.0115, 0.0021,\n",
       "         0.0117, 0.0012, 0.0014, 0.0021, 0.0117, 0.0134, 0.0070, 0.0070, 0.0014,\n",
       "         0.0091, 0.0021, 0.0043, 0.0014, 0.0091, 0.0115, 0.0070, 0.0098],\n",
       "        [0.0115, 0.0097, 0.0025, 0.0067, 0.0079, 0.0045, 0.0025, 0.0056, 0.0017,\n",
       "         0.0085, 0.0118, 0.0067, 0.0097, 0.0025, 0.0030, 0.0074, 0.0118, 0.0118,\n",
       "         0.0017, 0.0118, 0.0025, 0.0115, 0.0025, 0.0078, 0.0067, 0.0142, 0.0097,\n",
       "         0.0031, 0.0025, 0.0056, 0.0017, 0.0085, 0.0118, 0.0067, 0.0097, 0.0115,\n",
       "         0.0025, 0.0078, 0.0067, 0.0142, 0.0097, 0.0031, 0.0025, 0.0056, 0.0017,\n",
       "         0.0085, 0.0118, 0.0067, 0.0097, 0.0025, 0.0030, 0.0074, 0.0118, 0.0118,\n",
       "         0.0017, 0.0118, 0.0025, 0.0115, 0.0097, 0.0025, 0.0067, 0.0079, 0.0045,\n",
       "         0.0025, 0.0056, 0.0017, 0.0085, 0.0118, 0.0067, 0.0097, 0.0096, 0.0019,\n",
       "         0.0017, 0.0025, 0.0096, 0.0115, 0.0079, 0.0079, 0.0017, 0.0085, 0.0025,\n",
       "         0.0056, 0.0017, 0.0085, 0.0118, 0.0067, 0.0097, 0.0025, 0.0019, 0.0142,\n",
       "         0.0031, 0.0118, 0.0025, 0.0096, 0.0019, 0.0017, 0.0025, 0.0118, 0.0019,\n",
       "         0.0067, 0.0085, 0.0096, 0.0017, 0.0085, 0.0025, 0.0056, 0.0017, 0.0085,\n",
       "         0.0118, 0.0067, 0.0097, 0.0096, 0.0019, 0.0017, 0.0025, 0.0118, 0.0019,\n",
       "         0.0067, 0.0085, 0.0096, 0.0017, 0.0085, 0.0025, 0.0056, 0.0017, 0.0085,\n",
       "         0.0118, 0.0067, 0.0097, 0.0025, 0.0019, 0.0142, 0.0031, 0.0118, 0.0025,\n",
       "         0.0096, 0.0019, 0.0017, 0.0025, 0.0096, 0.0115, 0.0079, 0.0079, 0.0017,\n",
       "         0.0085, 0.0025, 0.0056, 0.0017, 0.0085, 0.0118, 0.0067, 0.0097],\n",
       "        [0.0127, 0.0100, 0.0023, 0.0069, 0.0070, 0.0046, 0.0023, 0.0049, 0.0013,\n",
       "         0.0094, 0.0113, 0.0069, 0.0100, 0.0023, 0.0024, 0.0074, 0.0113, 0.0113,\n",
       "         0.0013, 0.0113, 0.0023, 0.0127, 0.0023, 0.0068, 0.0069, 0.0139, 0.0100,\n",
       "         0.0032, 0.0023, 0.0049, 0.0013, 0.0094, 0.0113, 0.0069, 0.0100, 0.0127,\n",
       "         0.0023, 0.0068, 0.0069, 0.0139, 0.0100, 0.0032, 0.0023, 0.0049, 0.0013,\n",
       "         0.0094, 0.0113, 0.0069, 0.0100, 0.0023, 0.0024, 0.0074, 0.0113, 0.0113,\n",
       "         0.0013, 0.0113, 0.0023, 0.0127, 0.0100, 0.0023, 0.0069, 0.0070, 0.0046,\n",
       "         0.0023, 0.0049, 0.0013, 0.0094, 0.0113, 0.0069, 0.0100, 0.0112, 0.0015,\n",
       "         0.0013, 0.0023, 0.0112, 0.0127, 0.0070, 0.0070, 0.0013, 0.0094, 0.0023,\n",
       "         0.0049, 0.0013, 0.0094, 0.0113, 0.0069, 0.0100, 0.0023, 0.0015, 0.0139,\n",
       "         0.0032, 0.0113, 0.0023, 0.0112, 0.0015, 0.0013, 0.0023, 0.0113, 0.0015,\n",
       "         0.0069, 0.0094, 0.0112, 0.0013, 0.0094, 0.0023, 0.0049, 0.0013, 0.0094,\n",
       "         0.0113, 0.0069, 0.0100, 0.0112, 0.0015, 0.0013, 0.0023, 0.0113, 0.0015,\n",
       "         0.0069, 0.0094, 0.0112, 0.0013, 0.0094, 0.0023, 0.0049, 0.0013, 0.0094,\n",
       "         0.0113, 0.0069, 0.0100, 0.0023, 0.0015, 0.0139, 0.0032, 0.0113, 0.0023,\n",
       "         0.0112, 0.0015, 0.0013, 0.0023, 0.0112, 0.0127, 0.0070, 0.0070, 0.0013,\n",
       "         0.0094, 0.0023, 0.0049, 0.0013, 0.0094, 0.0113, 0.0069, 0.0100],\n",
       "        [0.0117, 0.0100, 0.0023, 0.0071, 0.0080, 0.0045, 0.0023, 0.0053, 0.0016,\n",
       "         0.0087, 0.0113, 0.0071, 0.0100, 0.0023, 0.0026, 0.0072, 0.0113, 0.0113,\n",
       "         0.0016, 0.0113, 0.0023, 0.0117, 0.0023, 0.0070, 0.0071, 0.0142, 0.0100,\n",
       "         0.0033, 0.0023, 0.0053, 0.0016, 0.0087, 0.0113, 0.0071, 0.0100, 0.0117,\n",
       "         0.0023, 0.0070, 0.0071, 0.0142, 0.0100, 0.0033, 0.0023, 0.0053, 0.0016,\n",
       "         0.0087, 0.0113, 0.0071, 0.0100, 0.0023, 0.0026, 0.0072, 0.0113, 0.0113,\n",
       "         0.0016, 0.0113, 0.0023, 0.0117, 0.0100, 0.0023, 0.0071, 0.0080, 0.0045,\n",
       "         0.0023, 0.0053, 0.0016, 0.0087, 0.0113, 0.0071, 0.0100, 0.0102, 0.0018,\n",
       "         0.0016, 0.0023, 0.0102, 0.0117, 0.0080, 0.0080, 0.0016, 0.0087, 0.0023,\n",
       "         0.0053, 0.0016, 0.0087, 0.0113, 0.0071, 0.0100, 0.0023, 0.0018, 0.0142,\n",
       "         0.0033, 0.0113, 0.0023, 0.0102, 0.0018, 0.0016, 0.0023, 0.0113, 0.0018,\n",
       "         0.0071, 0.0087, 0.0102, 0.0016, 0.0087, 0.0023, 0.0053, 0.0016, 0.0087,\n",
       "         0.0113, 0.0071, 0.0100, 0.0102, 0.0018, 0.0016, 0.0023, 0.0113, 0.0018,\n",
       "         0.0071, 0.0087, 0.0102, 0.0016, 0.0087, 0.0023, 0.0053, 0.0016, 0.0087,\n",
       "         0.0113, 0.0071, 0.0100, 0.0023, 0.0018, 0.0142, 0.0033, 0.0113, 0.0023,\n",
       "         0.0102, 0.0018, 0.0016, 0.0023, 0.0102, 0.0117, 0.0080, 0.0080, 0.0016,\n",
       "         0.0087, 0.0023, 0.0053, 0.0016, 0.0087, 0.0113, 0.0071, 0.0100]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"evals/grammar-winoground\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"evals/grammar-winoground/clip.npy\", softmax_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
