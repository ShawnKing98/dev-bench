{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookingWhileListeningDataset(Dataset):\n",
    "    def __init__(self, image_folder, pair_csv, transform=None, text_transform=None, k=2):\n",
    "        self.image_folder = image_folder\n",
    "        self.pairs = pd.read_csv(pair_csv)\n",
    "        # If no transform is provided, use ToTensor as default\n",
    "        self.transform = transform if transform is not None else transforms.ToTensor()\n",
    "        self.text_transform = text_transform\n",
    "        self.k = k\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.pairs.iloc[idx]\n",
    "        images, texts = [], []\n",
    "        for i in range(self.k):\n",
    "            image_filename = row[f'image{i+1}']\n",
    "            image_path = os.path.join(self.image_folder, image_filename)\n",
    "            image = Image.open(image_path)\n",
    "            image = self.transform(image)  # This will convert the PIL image to a tensor\n",
    "            images.append(image)\n",
    "\n",
    "            text = row[f'text{i+1}']\n",
    "            if self.text_transform:\n",
    "               text = self.text_transform(text)\n",
    "            texts.append(text)\n",
    "        return images, texts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, encoder, text_encoder, fusion_module, k=2):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.encoder = encoder  # For images\n",
    "        self.text_encoder = text_encoder  # For text\n",
    "        self.fusion_module = fusion_module\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        image_encodings = [self.encoder(image) for image in images]\n",
    "        if self.text_encoder:\n",
    "            text_encodings = [self.text_encoder(text) for text in texts]\n",
    "        else:\n",
    "            text_encodings = [None] * self.k\n",
    "        fused_encodings = [self.fusion_module(image_enc, text_enc) for image_enc, text_enc in zip(image_encodings, text_encodings)]\n",
    "        return fused_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_folder = \"C:\\\\Users\\\\sunny\\\\Desktop\\\\research\\\\cogeval\\\\tablet_images\\\\\"\n",
    "\n",
    "pair_csv = \"C:\\\\Users\\\\sunny\\\\Desktop\\\\research\\\\cogeval\\\\manifest.csv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LookingWhileListeningDataset(\n",
    "    image_folder= image_folder,\n",
    "    pair_csv= pair_csv,\n",
    "    transform=None, \n",
    "    text_transform=None,\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(batch):\n",
    "    images = [item for data in batch for item in data[0]]\n",
    "    texts = [item for data in batch for item in data[1]]\n",
    "    return images, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=False, collate_fn=collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_softmax_probs(data_loader, processor, model):\n",
    "    model.eval()  \n",
    "    softmax_probs = []\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for batch in data_loader:\n",
    "            images, texts = batch\n",
    "            # Flatten the list of texts if they are in tuples for each image\n",
    "            texts = [text for text_pair in texts for text in text_pair]\n",
    "            # Process the images and texts through the CLIP processor\n",
    "            # Convert the list of PIL Images to a list and then to a batch of tensors\n",
    "            inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "            outputs = model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "            probs = logits_per_image.softmax(dim=1)  # Softmax to get label probabilities\n",
    "            softmax_probs.extend(probs)\n",
    "    \n",
    "    # Convert the list of softmax probabilities to a tensor\n",
    "    return torch.stack(softmax_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_probs = calculate_softmax_probs(data_loader, processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0137, 0.0037, 0.0037,  ..., 0.0074, 0.0089, 0.0087],\n",
       "        [0.0141, 0.0039, 0.0039,  ..., 0.0076, 0.0082, 0.0095],\n",
       "        [0.0167, 0.0042, 0.0042,  ..., 0.0059, 0.0065, 0.0115],\n",
       "        ...,\n",
       "        [0.0137, 0.0039, 0.0039,  ..., 0.0076, 0.0088, 0.0090],\n",
       "        [0.0137, 0.0037, 0.0037,  ..., 0.0074, 0.0090, 0.0087],\n",
       "        [0.0137, 0.0038, 0.0038,  ..., 0.0074, 0.0090, 0.0086]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
