{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sunny\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\sunny\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\sunny\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookingWhileListeningDataset(Dataset):\n",
    "    def __init__(self, image_folder, pair_csv, transform=None, text_transform=None, k=4):\n",
    "        self.image_folder = image_folder\n",
    "        self.pairs = pd.read_csv(pair_csv)\n",
    "        self.transform = transform if transform is not None else transforms.ToTensor()\n",
    "        self.text_transform = text_transform\n",
    "        self.k = k\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.pairs.iloc[idx]\n",
    "        images = []\n",
    "        text = row['text1']\n",
    "        if self.text_transform:\n",
    "            text = self.text_transform(text)\n",
    "        # Only one text for all k images\n",
    "        for i in range(1, self.k + 1):  # Adjust index to start at 1\n",
    "            image_filename = row[f'image{i}']  # Adjusted to directly use i for indexing\n",
    "            image_path = os.path.join(self.image_folder, image_filename)\n",
    "            image = Image.open(image_path)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)  # This will convert the PIL image to a tensor\n",
    "            images.append(image)\n",
    "        return images, text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, encoder, text_encoder, fusion_module, k=4):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.encoder = encoder  # For images\n",
    "        self.text_encoder = text_encoder  # For text\n",
    "        self.fusion_module = fusion_module\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, images, text):\n",
    "        # Encode each image using the image encoder\n",
    "        image_encodings = [self.encoder(image) for image in images]\n",
    "        # Encode the text only once since there's only one text for all images\n",
    "        text_encoding = self.text_encoder(text) if self.text_encoder else None\n",
    "        # If text_encoder is None, replicate None k times for the loop below\n",
    "        text_encodings = [text_encoding] * self.k if text_encoding is not None else [None] * self.k\n",
    "        # Fuse each image encoding with the text encoding\n",
    "        fused_encodings = [self.fusion_module(image_enc, text_enc) for image_enc, text_enc in zip(image_encodings, text_encodings)]\n",
    "        return fused_encodings\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_folder = \"C:\\\\Users\\\\sunny\\\\Desktop\\\\research\\\\cogeval\\\\sample_items\\\\trog-ex\\\\\"\n",
    "\n",
    "pair_csv = \"C:\\\\Users\\\\sunny\\\\Desktop\\\\research\\\\cogeval\\\\sample_items\\\\trog-ex\\\\trog-manifest.csv\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LookingWhileListeningDataset(\n",
    "    image_folder= image_folder,\n",
    "    pair_csv= pair_csv,\n",
    "    transform=None, \n",
    "    text_transform=None,\n",
    "    k=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(batch):\n",
    "    images = [image for data in batch for image in data[0]]\n",
    "    # Since there is only one text per set of images, we don't need to flatten a list of texts\n",
    "    texts = [data[1] for data in batch]\n",
    "    return images, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=False, collate_fn=collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_softmax_probs(data_loader, processor, model):\n",
    "    model.eval()  \n",
    "    softmax_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, text in data_loader:  # Notice how we're expecting a single text here\n",
    "            # Repeat the single text k times (k = number of images per set)\n",
    "            texts = [text] * len(images)\n",
    "            # Process the images and texts through the CLIP processor\n",
    "            inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "            outputs = model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "            probs = logits_per_image.softmax(dim=1)  # Softmax to get probabilities\n",
    "            softmax_probs.extend(probs)\n",
    "    \n",
    "    return torch.stack(softmax_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    }
   ],
   "source": [
    "softmax_probs = calculate_softmax_probs(data_loader, processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"evals/grammar-trog\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"evals/grammar-trog/clip.npy\", softmax_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
