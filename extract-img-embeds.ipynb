{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from PIL import Image\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasets import Dataset, load_dataset, Image\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEM_VOC_ROOT = \"assets/sem-viz_obj_cat/\"\n",
    "sem_voc_manifest = pd.read_csv(SEM_VOC_ROOT + \"manifest.csv\")\n",
    "sem_voc_manifest[\"image\"] = [SEM_VOC_ROOT+i for i in sem_voc_manifest[\"image\"]]\n",
    "sem_voc_ds = Dataset.from_pandas(sem_voc_manifest).cast_column(\"image\", Image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_collator(data):\n",
    "    return {k: [ex[k] for ex in data] for k in data[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_voc_dl = DataLoader(sem_voc_ds, batch_size=16, collate_fn=image_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feats(dataloader, processor, model):\n",
    "    all_feats = []\n",
    "    for d in dataloader:\n",
    "        inputs = processor(images=d[\"image\"], return_tensors=\"pt\")\n",
    "        feats = model.get_image_features(**inputs).detach().numpy()\n",
    "        all_feats.append(feats)\n",
    "    return np.concatenate(all_feats, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = get_image_feats(sem_voc_dl, processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: make robust based on manifest instead of hand coding\n",
    "\n",
    "NUM_CATS = 8\n",
    "NUM_IMGS = 9\n",
    "\n",
    "feats_mean = np.array([np.mean(feats[c*NUM_IMGS:((c+1)*NUM_IMGS)-1], axis=0) for c in range(NUM_CATS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sims = cosine_similarity(feats_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999994, 0.72129065, 0.74828994, 0.7112463 , 0.6963773 ,\n",
       "        0.73268056, 0.7028516 , 0.70743144],\n",
       "       [0.72129065, 0.9999998 , 0.8280212 , 0.828451  , 0.7770638 ,\n",
       "        0.81577206, 0.8138706 , 0.8407951 ],\n",
       "       [0.74828994, 0.8280212 , 1.0000001 , 0.9278395 , 0.7848074 ,\n",
       "        0.8789254 , 0.7716011 , 0.81575674],\n",
       "       [0.7112463 , 0.828451  , 0.9278395 , 1.0000001 , 0.79224056,\n",
       "        0.87045276, 0.8131415 , 0.848372  ],\n",
       "       [0.6963773 , 0.7770638 , 0.7848074 , 0.79224056, 0.9999996 ,\n",
       "        0.8426869 , 0.84862626, 0.81467444],\n",
       "       [0.73268056, 0.81577206, 0.8789254 , 0.87045276, 0.8426869 ,\n",
       "        1.0000006 , 0.82269806, 0.87957287],\n",
       "       [0.7028516 , 0.8138706 , 0.7716011 , 0.8131415 , 0.84862626,\n",
       "        0.82269806, 0.9999995 , 0.89862996],\n",
       "       [0.70743144, 0.8407951 , 0.81575674, 0.848372  , 0.81467444,\n",
       "        0.87957287, 0.89862996, 0.9999989 ]], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"evals/sem-viz_obj_cat/clip.npy\", img_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEM_THINGS_ROOT = \"assets/sem-things/\"\n",
    "sem_things_manifest = pd.read_csv(SEM_THINGS_ROOT + \"manifest.csv\")\n",
    "sem_things_manifest[\"image\"] = [SEM_THINGS_ROOT+i for i in sem_things_manifest[\"image\"]]\n",
    "sem_things_ds = Dataset.from_pandas(sem_things_manifest).cast_column(\"image\", Image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_things_dl = DataLoader(sem_things_ds, batch_size=16, collate_fn=image_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "things_feats = get_image_feats(sem_things_dl, processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "things_sims = cosine_similarity(things_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"evals/sem-things\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"evals/sem-things/clip.npy\", things_sims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs330hw0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
